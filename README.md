üìä Modelos Avan√ßados ‚Äì Machine Learning

Este reposit√≥rio re√∫ne implementa√ß√µes pr√°ticas e exerc√≠cios de aprendizado de m√°quina, explorando desde modelos supervisionados como Regress√£o Log√≠stica, √Årvores de Decis√£o e Random Forest at√© t√©cnicas n√£o supervisionadas como K-Means.

O objetivo √© consolidar conceitos de classifica√ß√£o, avalia√ß√£o de modelos e clustering, aplicados em diferentes contextos (neg√≥cios, sa√∫de e biologia).

üöÄ Objetivos Gerais

Praticar a implementa√ß√£o de algoritmos de Machine Learning em Python.

Explorar diferentes abordagens para problemas de classifica√ß√£o e agrupamento.

Comparar modelos e avaliar m√©tricas de desempenho.

Desenvolver habilidades em pr√©-processamento, an√°lise explorat√≥ria e valida√ß√£o de modelos.

üõ†Ô∏è Tecnologias Utilizadas

Python

Pandas

NumPy

Scikit-learn

Matplotlib

Seaborn

Imbalanced-learn (para balanceamento de classes)

üìÇ Estrutura do Projeto
‚úÖ 1. 1_Implementa√ß√£o_em_Python_Regress√£o_log√≠stica_2_.ipynb

Implementa√ß√£o pr√°tica da Regress√£o Log√≠stica em um caso de leads de vendas de uma empresa automotiva.

‚úÖ 2. 2_Exerc√≠cio_Projeto_de_Doen√ßas_Cardiovasculares_Regress√£o_Log√≠stica.ipynb

Exerc√≠cio aplicado em dados reais de sa√∫de, prevendo risco de doen√ßas cardiovasculares com regress√£o log√≠stica.

‚úÖ 3. 3_Introdu√ß√£o_√†_duelo_entre_Modelos_√Årvore_e_Regress√£o.ipynb

Compara√ß√£o entre √Årvore de Decis√£o e Regress√£o Log√≠stica, avaliando vantagens e limita√ß√µes de cada modelo.

‚úÖ 4. 4_Introdu√ß√£o_ao_K_means.ipynb

Introdu√ß√£o ao algoritmo K-Means, explorando fundamentos, implementa√ß√£o e visualiza√ß√£o de clusters.

‚úÖ 5. 5_Exerc√≠cio_K_Means.ipynb

Aplica√ß√£o pr√°tica do K-Means em dados biol√≥gicos (penguins), segmentando esp√©cies com base em caracter√≠sticas f√≠sicas.

‚úÖ 6. 6_Random_Forest_e_Hiperpar√¢metros_Classifica√ß√£o.ipynb

Implementa√ß√£o de Random Forest para classifica√ß√£o, utilizando a base de leads da loja automobil√≠stica. Inclui:

Compara√ß√£o com Regress√£o Log√≠stica

Ajuste de hiperpar√¢metros com RandomizedSearchCV

Avalia√ß√£o de m√©tricas (acur√°cia, matriz de confus√£o, classification report)

‚úÖ 7. 7_Exerc√≠cio_Random_Forest.ipynb

Exerc√≠cio pr√°tico com base de qualidade de vinhos, prevendo a pontua√ß√£o sensorial usando Random Forest para classifica√ß√£o multiclasse.
Inclui:

An√°lise explorat√≥ria dos dados

Constru√ß√£o do modelo

Ajuste de hiperpar√¢metros

Avalia√ß√£o do desempenho e import√¢ncia das vari√°veis

‚úÖ 8. 8_Duelo_entre_Modelos_√Årvore_e_Random_Forest.ipynb

Compara√ß√£o pr√°tica entre √Årvore de Decis√£o e Random Forest, aplicando:

Ajuste de hiperpar√¢metros com GridSearchCV

SMOTE para balanceamento de classes

Avalia√ß√£o com m√©tricas avan√ßadas (ROC AUC, curva ROC, matriz de confus√£o)

üìå Pr√≥ximos Passos

Incluir outros algoritmos supervisionados (SVM, Gradient Boosting, XGBoost).

Avaliar t√©cnicas avan√ßadas de clustering (DBSCAN, Hier√°rquico).

Expandir an√°lises com valida√ß√£o cruzada e tuning de hiperpar√¢metros.

Integrar explicabilidade de modelos (Feature Importance, SHAP, LIME).


Integrar explicabilidade de modelos (feature importance, SHAP).
